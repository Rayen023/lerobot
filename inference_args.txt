usage: record.py [-h] [--config_path str] [--robot str] [--robot.type {hope_jr_hand,hope_jr_arm,koch_follower,so100_follower,so100_follower_end_effector,so101_follower}] [--robot.side str] [--robot.urdf_path [str]] [--robot.target_frame_name str]
                 [--robot.end_effector_bounds Dict] [--robot.max_gripper_pos float] [--robot.end_effector_step_sizes Dict] [--robot.id [str]] [--robot.calibration_dir [Path]] [--robot.port str] [--robot.disable_torque_on_disconnect bool]
                 [--robot.max_relative_target [int]] [--robot.cameras Dict] [--robot.use_degrees bool] [--dataset str] [--dataset.repo_id str] [--dataset.single_task str] [--dataset.root [str|Path]] [--dataset.fps int] [--dataset.episode_time_s int|float]
                 [--dataset.reset_time_s int|float] [--dataset.num_episodes int] [--dataset.video bool] [--dataset.push_to_hub bool] [--dataset.private bool] [--dataset.tags [List]] [--dataset.num_image_writer_processes int]
                 [--dataset.num_image_writer_threads_per_camera int] [--teleop str] [--teleop.type {homunculus_glove,homunculus_arm,koch_leader,so100_leader,so101_leader,keyboard,keyboard_ee}] [--teleop.side str] [--teleop.baud_rate int]
                 [--teleop.gripper_open_pos float] [--teleop.port str] [--teleop.use_degrees bool] [--teleop.id [str]] [--teleop.calibration_dir [Path]] [--teleop.mock bool] [--teleop.use_gripper bool] [--policy str]
                 [--policy.type {act,diffusion,pi0,smolvla,tdmpc,vqbet,pi0fast,sac,reward_classifier}] [--policy.replace_final_stride_with_dilation int] [--policy.pre_norm bool] [--policy.dim_model int] [--policy.n_heads int] [--policy.dim_feedforward int]
                 [--policy.feedforward_activation str] [--policy.n_encoder_layers int] [--policy.n_decoder_layers int] [--policy.use_vae bool] [--policy.n_vae_encoder_layers int] [--policy.temporal_ensemble_coeff [float]] [--policy.kl_weight float]
                 [--policy.optimizer_lr_backbone float] [--policy.drop_n_last_frames int] [--policy.use_separate_rgb_encoder_per_camera bool] [--policy.down_dims int [int, ...]] [--policy.kernel_size int] [--policy.n_groups int]
                 [--policy.diffusion_step_embed_dim int] [--policy.use_film_scale_modulation bool] [--policy.noise_scheduler_type str] [--policy.num_train_timesteps int] [--policy.beta_schedule str] [--policy.beta_start float] [--policy.beta_end float]
                 [--policy.prediction_type str] [--policy.clip_sample bool] [--policy.clip_sample_range float] [--policy.num_inference_steps [int]] [--policy.do_mask_loss_for_padding bool] [--policy.scheduler_name str]
                 [--policy.attention_implementation str] [--policy.num_steps int] [--policy.train_expert_only bool] [--policy.train_state_proj bool] [--policy.optimizer_grad_clip_norm float] [--policy.vlm_model_name str] [--policy.load_vlm_weights bool]
                 [--policy.add_image_special_tokens bool] [--policy.attention_mode str] [--policy.prefix_length int] [--policy.pad_language_to str] [--policy.num_expert_layers int] [--policy.num_vlm_layers int] [--policy.self_attn_every_n_layers int]
                 [--policy.expert_width_multiplier float] [--policy.min_period float] [--policy.max_period float] [--policy.n_action_repeats int] [--policy.horizon int] [--policy.q_ensemble_size int] [--policy.mlp_dim int] [--policy.use_mpc bool]
                 [--policy.cem_iterations int] [--policy.max_std float] [--policy.min_std float] [--policy.n_gaussian_samples int] [--policy.n_pi_samples int] [--policy.uncertainty_regularizer_coeff float] [--policy.n_elites int]
                 [--policy.elite_weighting_temperature float] [--policy.gaussian_mean_momentum float] [--policy.max_random_shift_ratio float] [--policy.reward_coeff float] [--policy.expectile_weight float] [--policy.value_coeff float]
                 [--policy.consistency_coeff float] [--policy.advantage_scaling float] [--policy.pi_coeff float] [--policy.temporal_decay_coeff float] [--policy.target_model_momentum float] [--policy.n_action_pred_token int]
                 [--policy.action_chunk_size int] [--policy.vision_backbone str] [--policy.crop_shape [int int]] [--policy.crop_is_random bool] [--policy.pretrained_backbone_weights [str]] [--policy.use_group_norm bool]
                 [--policy.spatial_softmax_num_keypoints int] [--policy.n_vqvae_training_steps int] [--policy.vqvae_n_embed int] [--policy.vqvae_embedding_dim int] [--policy.vqvae_enc_hidden_dim int] [--policy.gpt_block_size int]
                 [--policy.gpt_input_dim int] [--policy.gpt_output_dim int] [--policy.gpt_n_layer int] [--policy.gpt_n_head int] [--policy.gpt_hidden_dim int] [--policy.dropout float] [--policy.mlp_hidden_dim int] [--policy.offset_loss_weight float]
                 [--policy.primary_code_loss_weight float] [--policy.secondary_code_loss_weight float] [--policy.bet_softmax_temperature float] [--policy.sequentially_select bool] [--policy.optimizer_vqvae_lr float]
                 [--policy.optimizer_vqvae_weight_decay float] [--policy.chunk_size int] [--policy.n_action_steps int] [--policy.max_state_dim int] [--policy.max_action_dim int] [--policy.resize_imgs_with_padding int int]
                 [--policy.interpolate_like_pi bool] [--policy.empty_cameras int] [--policy.adapt_to_pi_aloha bool] [--policy.use_delta_joint_actions_aloha bool] [--policy.tokenizer_max_length int] [--policy.proj_width int]
                 [--policy.max_decoding_steps int] [--policy.fast_skip_tokens int] [--policy.max_input_seq_len int] [--policy.use_cache bool] [--policy.freeze_lm_head bool] [--policy.optimizer_lr float] [--policy.optimizer_betas float float]
                 [--policy.optimizer_eps float] [--policy.optimizer_weight_decay float] [--policy.scheduler_warmup_steps int] [--policy.scheduler_decay_steps int] [--policy.scheduler_decay_lr float] [--policy.checkpoint_path str]
                 [--policy.padding_side str] [--policy.precision str] [--policy.relaxed_action_decoding bool] [--policy.dataset_stats [Dict]] [--policy.storage_device str] [--policy.vision_encoder_name [str]] [--policy.freeze_vision_encoder bool]
                 [--policy.image_encoder_hidden_dim int] [--policy.shared_encoder bool] [--policy.num_discrete_actions [int]] [--policy.online_steps int] [--policy.online_env_seed int] [--policy.online_buffer_capacity int]
                 [--policy.offline_buffer_capacity int] [--policy.async_prefetch bool] [--policy.online_step_before_learning int] [--policy.policy_update_freq int] [--policy.discount float] [--policy.temperature_init float] [--policy.num_critics int]
                 [--policy.num_subsample_critics [int]] [--policy.critic_lr float] [--policy.actor_lr float] [--policy.temperature_lr float] [--policy.critic_target_update_weight float] [--policy.utd_ratio int] [--policy.state_encoder_hidden_dim int]
                 [--policy.target_entropy [float]] [--policy.use_backup_entropy bool] [--critic_network_kwargs str] [--policy.critic_network_kwargs.hidden_dims List] [--policy.critic_network_kwargs.activate_final bool]
                 [--policy.critic_network_kwargs.final_activation [str]] [--actor_network_kwargs str] [--policy.actor_network_kwargs.hidden_dims List] [--policy.actor_network_kwargs.activate_final bool] [--policy_kwargs str]
                 [--policy.policy_kwargs.use_tanh_squash bool] [--policy.policy_kwargs.std_min float] [--policy.policy_kwargs.std_max float] [--policy.policy_kwargs.init_final float] [--discrete_critic_network_kwargs str]
                 [--policy.discrete_critic_network_kwargs.hidden_dims List] [--policy.discrete_critic_network_kwargs.activate_final bool] [--policy.discrete_critic_network_kwargs.final_activation [str]] [--actor_learner_config str]
                 [--policy.actor_learner_config.learner_host str] [--policy.actor_learner_config.learner_port int] [--policy.actor_learner_config.policy_parameters_push_frequency int] [--policy.actor_learner_config.queue_get_timeout float]
                 [--concurrency str] [--policy.concurrency.actor str] [--policy.concurrency.learner str] [--policy.use_torch_compile bool] [--policy.n_obs_steps int] [--policy.normalization_mapping Dict] [--policy.input_features Dict]
                 [--policy.output_features Dict] [--policy.device str] [--policy.use_amp bool] [--policy.push_to_hub bool] [--policy.repo_id [str]] [--policy.private [bool]] [--policy.tags [List]] [--policy.license [str]] [--policy.name str]
                 [--policy.num_classes int] [--policy.hidden_dim int] [--policy.latent_dim int] [--policy.image_embedding_pooling_dim int] [--policy.dropout_rate float] [--policy.model_name str] [--policy.model_type str] [--policy.num_cameras int]
                 [--policy.learning_rate float] [--policy.weight_decay float] [--policy.grad_clip_norm float] [--display_data bool] [--play_sounds bool] [--resume bool]

options:
  -h, --help            show this help message and exit
  --config_path str     Path for a config file to parse with draccus (default: None)
  --robot str           Config file for robot (default: None)
  --dataset str         Config file for dataset (default: None)
  --teleop str          Config file for teleop (default: None)
  --policy str          Config file for policy (default: None)
  --critic_network_kwargs str
                        Config file for critic_network_kwargs (default: None)
  --actor_network_kwargs str
                        Config file for actor_network_kwargs (default: None)
  --policy_kwargs str   Config file for policy_kwargs (default: None)
  --discrete_critic_network_kwargs str
                        Config file for discrete_critic_network_kwargs (default: None)
  --actor_learner_config str
                        Config file for actor_learner_config (default: None)
  --concurrency str     Config file for concurrency (default: None)

RecordConfig:

  --display_data bool   Display all cameras on screen (default: False)
  --play_sounds bool    Use vocal synthesis to read events. (default: True)
  --resume bool         Resume recording on an existing dataset. (default: False)

RobotConfig ['robot']:

  --robot.type {hope_jr_hand,hope_jr_arm,koch_follower,so100_follower,so100_follower_end_effector,so101_follower}
                        Which type of RobotConfig ['robot'] to use (default: None)

HopeJrHandConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the hand (default: None)
  --robot.side str      "left" / "right" (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.cameras Dict  

HopeJrArmConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the hand (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.max_relative_target [int]
                        `max_relative_target` limits the magnitude of the relative positional target vector for safety purposes. Set this to a positive scalar to have the same value for all motors, or a list that is the same length as the number of motors
                        in your follower arms. (default: None)
  --robot.cameras Dict  

KochFollowerConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the arm (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.max_relative_target [int]
                        `max_relative_target` limits the magnitude of the relative positional target vector for safety purposes. Set this to a positive scalar to have the same value for all motors, or a list that is the same length as the number of motors
                        in your follower arms. (default: None)
  --robot.cameras Dict  cameras (default: {})
  --robot.use_degrees bool
                        Set to `True` for backward compatibility with previous policies/dataset (default: False)

SO100FollowerConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the arm (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.max_relative_target [int]
                        `max_relative_target` limits the magnitude of the relative positional target vector for safety purposes. Set this to a positive scalar to have the same value for all motors, or a list that is the same length as the number of motors
                        in your follower arms. (default: None)
  --robot.cameras Dict  cameras (default: {})
  --robot.use_degrees bool
                        Set to `True` for backward compatibility with previous policies/dataset (default: False)

SO100FollowerEndEffectorConfig ['robot']:
  Configuration for the SO100FollowerEndEffector robot.

  --robot.id [str]      Allows to distinguish between different robots of the same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the arm (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.max_relative_target [int]
                        `max_relative_target` limits the magnitude of the relative positional target vector for safety purposes. Set this to a positive scalar to have the same value for all motors, or a list that is the same length as the number of motors
                        in your follower arms. (default: None)
  --robot.cameras Dict  cameras (default: {})
  --robot.use_degrees bool
                        Set to `True` for backward compatibility with previous policies/dataset (default: False)
  --robot.urdf_path [str]
                        Path to URDF file for kinematics NOTE: It is highly recommended to use the urdf in the SO-ARM100 repo: https://github.com/TheRobotStudio/SO-ARM100/blob/main/Simulation/SO101/so101_new_calib.urdf (default: None)
  --robot.target_frame_name str
                        End-effector frame name in the URDF (default: gripper_frame_link)
  --robot.end_effector_bounds Dict
                        Default bounds for the end-effector position (in meters) (default: {'min': [-1.0, -1.0, -1.0], 'max': [1.0, 1.0, 1.0]})
  --robot.max_gripper_pos float
  --robot.end_effector_step_sizes Dict

SO101FollowerConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the arm (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.max_relative_target [int]
                        `max_relative_target` limits the magnitude of the relative positional target vector for safety purposes. Set this to a positive scalar to have the same value for all motors, or a list that is the same length as the number of motors
                        in your follower arms. (default: None)
  --robot.cameras Dict  cameras (default: {})
  --robot.use_degrees bool
                        Set to `True` for backward compatibility with previous policies/dataset (default: False)

DatasetRecordConfig ['dataset']:

  --dataset.repo_id str
                        Dataset identifier. By convention it should match '{hf_username}/{dataset_name}' (e.g. `lerobot/test`). (default: None)
  --dataset.single_task str
                        A short but accurate description of the task performed during the recording (e.g. "Pick the Lego block and drop it in the box on the right.") (default: None)
  --dataset.root [str|Path]
                        Root directory where the dataset will be stored (e.g. 'dataset/path'). (default: None)
  --dataset.fps int     Limit the frames per second. (default: 30)
  --dataset.episode_time_s int|float
                        Number of seconds for data recording for each episode. (default: 60)
  --dataset.reset_time_s int|float
                        Number of seconds for resetting the environment after each episode. (default: 60)
  --dataset.num_episodes int
                        Number of episodes to record. (default: 50)
  --dataset.video bool  Encode frames in the dataset into video (default: True)
  --dataset.push_to_hub bool
                        Upload dataset to Hugging Face hub. (default: True)
  --dataset.private bool
                        Upload on private repository on the Hugging Face hub. (default: False)
  --dataset.tags [List]
                        Add tags to your dataset on the hub. (default: None)
  --dataset.num_image_writer_processes int
                        Number of subprocesses handling the saving of frames as PNG. Set to 0 to use threads only; set to â‰¥1 to use subprocesses, each using threads to write images. The best number of processes and threads depends on your system. We
                        recommend 4 threads per camera with 0 processes. If fps is unstable, adjust the thread count. If still unstable, try using 1 or more subprocesses. (default: 0)
  --dataset.num_image_writer_threads_per_camera int
                        Number of threads writing the frames as png images on disk, per camera. Too many threads might cause unstable teleoperation fps due to main thread being blocked. Not enough threads might cause low camera fps. (default: 4)

Optional ['teleop']:
  Whether to control the robot with a teleoperator

TeleoperatorConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.type {homunculus_glove,homunculus_arm,koch_leader,so100_leader,so101_leader,keyboard,keyboard_ee}
                        Which type of TeleoperatorConfig ['teleop'] to use (default: None)

HomunculusGloveConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the glove (default: None)
  --teleop.side str     "left" / "right" (default: None)
  --teleop.baud_rate int

HomunculusArmConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the arm (default: None)
  --teleop.baud_rate int

KochLeaderConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the arm (default: None)
  --teleop.gripper_open_pos float
                        Sets the arm in torque mode with the gripper motor set to this value. This makes it possible to squeeze the gripper and have it spring back to an open position on its own. (default: 50.0)

SO100LeaderConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the arm (default: None)

SO101LeaderConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the arm (default: None)
  --teleop.use_degrees bool

KeyboardTeleopConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.mock bool    TODO(Steven): Consider setting in here the keys that we want to capture/listen (default: False)

KeyboardEndEffectorTeleopConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.mock bool    TODO(Steven): Consider setting in here the keys that we want to capture/listen (default: False)
  --teleop.use_gripper bool

Optional ['policy']:
  Whether to control the robot with a policy

PreTrainedConfig ['policy']:
  Whether to control the robot with a policy

  --policy.type {act,diffusion,pi0,smolvla,tdmpc,vqbet,pi0fast,sac,reward_classifier}
                        Which type of PreTrainedConfig ['policy'] to use (default: None)

ACTConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        cuda | cpu | mp (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed Precision (AMP) for training and evaluation. With AMP, automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub. (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.chunk_size int
  --policy.n_action_steps int
  --policy.vision_backbone str
  --policy.pretrained_backbone_weights [str]
  --policy.replace_final_stride_with_dilation int
  --policy.pre_norm bool
  --policy.dim_model int
  --policy.n_heads int  
  --policy.dim_feedforward int
  --policy.feedforward_activation str
  --policy.n_encoder_layers int
  --policy.n_decoder_layers int
  --policy.use_vae bool
  --policy.latent_dim int
  --policy.n_vae_encoder_layers int
  --policy.temporal_ensemble_coeff [float]
  --policy.dropout float
  --policy.kl_weight float
  --policy.optimizer_lr float
                        Training preset (default: 1e-05)
  --policy.optimizer_weight_decay float
  --policy.optimizer_lr_backbone float

DiffusionConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        cuda | cpu | mp (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed Precision (AMP) for training and evaluation. With AMP, automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub. (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.horizon int  
  --policy.n_action_steps int
  --policy.drop_n_last_frames int
                        horizon - n_action_steps - n_obs_steps + 1 (default: 7)
  --policy.vision_backbone str
  --policy.crop_shape [int int]
  --policy.crop_is_random bool
  --policy.pretrained_backbone_weights [str]
  --policy.use_group_norm bool
  --policy.spatial_softmax_num_keypoints int
  --policy.use_separate_rgb_encoder_per_camera bool
  --policy.down_dims int [int, ...]
  --policy.kernel_size int
  --policy.n_groups int
  --policy.diffusion_step_embed_dim int
  --policy.use_film_scale_modulation bool
  --policy.noise_scheduler_type str
                        Noise scheduler. (default: DDPM)
  --policy.num_train_timesteps int
  --policy.beta_schedule str
  --policy.beta_start float
  --policy.beta_end float
  --policy.prediction_type str
  --policy.clip_sample bool
  --policy.clip_sample_range float
  --policy.num_inference_steps [int]
  --policy.do_mask_loss_for_padding bool
  --policy.optimizer_lr float
                        Training presets (default: 0.0001)
  --policy.optimizer_betas Any
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.scheduler_name str
  --policy.scheduler_warmup_steps int

PI0Config ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
                        Input / output structure. (default: 1)
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        cuda | cpu | mp (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed Precision (AMP) for training and evaluation. With AMP, automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub. (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.chunk_size int
  --policy.n_action_steps int
  --policy.max_state_dim int
                        Shorter state and action vectors will be padded (default: 32)
  --policy.max_action_dim int
  --policy.resize_imgs_with_padding int int
                        Image preprocessing (default: (224, 224))
  --policy.empty_cameras int
                        Add empty images. Used by pi0_aloha_sim which adds the empty left and right wrist cameras in addition to the top camera. (default: 0)
  --policy.adapt_to_pi_aloha bool
                        Converts the joint and gripper values from the standard Aloha space to the space used by the pi internal runtime which was used to train the base model. (default: False)
  --policy.use_delta_joint_actions_aloha bool
                        Converts joint dimensions to deltas with respect to the current state before passing to the model. Gripper dimensions will remain in absolute values. (default: False)
  --policy.tokenizer_max_length int
                        Tokenizer (default: 48)
  --policy.proj_width int
                        Projector (default: 1024)
  --policy.num_steps int
                        Decoding (default: 10)
  --policy.use_cache bool
                        Attention utils (default: True)
  --policy.attention_implementation str
                        or fa2, flex (default: eager)
  --policy.freeze_vision_encoder bool
                        Finetuning settings (default: True)
  --policy.train_expert_only bool
  --policy.train_state_proj bool
  --policy.optimizer_lr float
                        Training presets (default: 2.5e-05)
  --policy.optimizer_betas float float
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.scheduler_warmup_steps int
  --policy.scheduler_decay_steps int
  --policy.scheduler_decay_lr float

SmolVLAConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
                        Input / output structure. (default: 1)
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        cuda | cpu | mp (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed Precision (AMP) for training and evaluation. With AMP, automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub. (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.chunk_size int
  --policy.n_action_steps int
  --policy.max_state_dim int
                        Shorter state and action vectors will be padded (default: 32)
  --policy.max_action_dim int
  --policy.resize_imgs_with_padding int int
                        Image preprocessing (default: (512, 512))
  --policy.empty_cameras int
                        Add empty images. Used by smolvla_aloha_sim which adds the empty left and right wrist cameras in addition to the top camera. (default: 0)
  --policy.adapt_to_pi_aloha bool
                        Converts the joint and gripper values from the standard Aloha space to the space used by the pi internal runtime which was used to train the base model. (default: False)
  --policy.use_delta_joint_actions_aloha bool
                        Converts joint dimensions to deltas with respect to the current state before passing to the model. Gripper dimensions will remain in absolute values. (default: False)
  --policy.tokenizer_max_length int
                        Tokenizer (default: 48)
  --policy.num_steps int
                        Decoding (default: 10)
  --policy.use_cache bool
                        Attention utils (default: True)
  --policy.freeze_vision_encoder bool
                        Finetuning settings (default: True)
  --policy.train_expert_only bool
  --policy.train_state_proj bool
  --policy.optimizer_lr float
                        Training presets (default: 0.0001)
  --policy.optimizer_betas float float
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.optimizer_grad_clip_norm float
  --policy.scheduler_warmup_steps int
  --policy.scheduler_decay_steps int
  --policy.scheduler_decay_lr float
  --policy.vlm_model_name str
                        Select the VLM backbone. (default: HuggingFaceTB/SmolVLM2-500M-Video-Instruct)
  --policy.load_vlm_weights bool
                        Set to True in case of training the expert from scratch. True when init from pretrained SmolVLA weights (default: False)
  --policy.add_image_special_tokens bool
                        Whether to use special image tokens around image features. (default: False)
  --policy.attention_mode str
  --policy.prefix_length int
  --policy.pad_language_to str
                        "max_length" (default: longest)
  --policy.num_expert_layers int
                        Less or equal to 0 is the default where the action expert has the same number of layers of VLM. Otherwise the expert have less layers. (default: -1)
  --policy.num_vlm_layers int
                        Number of layers used in the VLM (first num_vlm_layers layers) (default: 16)
  --policy.self_attn_every_n_layers int
                        Interleave SA layers each self_attn_every_n_layers (default: 2)
  --policy.expert_width_multiplier float
                        The action expert hidden size (wrt to the VLM) (default: 0.75)
  --policy.min_period float
                        sensitivity range for the timestep used in sine-cosine positional encoding (default: 0.004)
  --policy.max_period float

TDMPCConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
                        Input / output structure. (default: 1)
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        cuda | cpu | mp (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed Precision (AMP) for training and evaluation. With AMP, automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub. (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.n_action_repeats int
  --policy.horizon int  
  --policy.n_action_steps int
  --policy.image_encoder_hidden_dim int
  --policy.state_encoder_hidden_dim int
  --policy.latent_dim int
  --policy.q_ensemble_size int
  --policy.mlp_dim int  
  --policy.discount float
  --policy.use_mpc bool
  --policy.cem_iterations int
  --policy.max_std float
  --policy.min_std float
  --policy.n_gaussian_samples int
  --policy.n_pi_samples int
  --policy.uncertainty_regularizer_coeff float
  --policy.n_elites int
  --policy.elite_weighting_temperature float
  --policy.gaussian_mean_momentum float
  --policy.max_random_shift_ratio float
  --policy.reward_coeff float
  --policy.expectile_weight float
  --policy.value_coeff float
  --policy.consistency_coeff float
  --policy.advantage_scaling float
  --policy.pi_coeff float
  --policy.temporal_decay_coeff float
  --policy.target_model_momentum float
  --policy.optimizer_lr float
                        Training presets (default: 0.0003)

VQBeTConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        cuda | cpu | mp (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed Precision (AMP) for training and evaluation. With AMP, automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub. (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.n_action_pred_token int
  --policy.action_chunk_size int
  --policy.vision_backbone str
  --policy.crop_shape [int int]
  --policy.crop_is_random bool
  --policy.pretrained_backbone_weights [str]
  --policy.use_group_norm bool
  --policy.spatial_softmax_num_keypoints int
  --policy.n_vqvae_training_steps int
  --policy.vqvae_n_embed int
  --policy.vqvae_embedding_dim int
  --policy.vqvae_enc_hidden_dim int
  --policy.gpt_block_size int
  --policy.gpt_input_dim int
  --policy.gpt_output_dim int
  --policy.gpt_n_layer int
  --policy.gpt_n_head int
  --policy.gpt_hidden_dim int
  --policy.dropout float
  --policy.mlp_hidden_dim int
  --policy.offset_loss_weight float
  --policy.primary_code_loss_weight float
  --policy.secondary_code_loss_weight float
  --policy.bet_softmax_temperature float
  --policy.sequentially_select bool
  --policy.optimizer_lr float
                        Training presets (default: 0.0001)
  --policy.optimizer_betas Any
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.optimizer_vqvae_lr float
  --policy.optimizer_vqvae_weight_decay float
  --policy.scheduler_warmup_steps int

PI0FASTConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
                        Input / output structure. (default: 1)
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        cuda | cpu | mp (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed Precision (AMP) for training and evaluation. With AMP, automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub. (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.chunk_size int
  --policy.n_action_steps int
  --policy.max_state_dim int
                        32 (default: 32)
  --policy.max_action_dim int
                        32 (default: 32)
  --policy.resize_imgs_with_padding int int
                        Image preprocessing (default: (224, 224))
  --policy.interpolate_like_pi bool
  --policy.empty_cameras int
                        Add empty images. Used by pi0_aloha_sim which adds the empty left and right wrist cameras in addition to the top camera. (default: 0)
  --policy.adapt_to_pi_aloha bool
                        Converts the joint and gripper values from the standard Aloha space to the space used by the pi internal runtime which was used to train the base model. (default: False)
  --policy.use_delta_joint_actions_aloha bool
                        Converts joint dimensions to deltas with respect to the current state before passing to the model. Gripper dimensions will remain in absolute values. (default: False)
  --policy.tokenizer_max_length int
                        Tokenizer (default: 48)
  --policy.proj_width int
                        Projector (default: 1024)
  --policy.max_decoding_steps int
                        Decoding (default: 256)
  --policy.fast_skip_tokens int
                        Skip last 128 tokens in PaliGemma vocab since they are special tokens (default: 128)
  --policy.max_input_seq_len int
                        512 (default: 256)
  --policy.use_cache bool
                        Utils (default: True)
  --policy.freeze_vision_encoder bool
                        Frozen parameters (default: True)
  --policy.freeze_lm_head bool
  --policy.optimizer_lr float
                        Training presets (default: 0.0001)
  --policy.optimizer_betas float float
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.scheduler_warmup_steps int
  --policy.scheduler_decay_steps int
  --policy.scheduler_decay_lr float
  --policy.checkpoint_path str
  --policy.padding_side str
  --policy.precision str
  --policy.grad_clip_norm float
  --policy.relaxed_action_decoding bool
                        Allows padding/truncation of generated action tokens during detokenization to ensure decoding. In the original version, tensors of 0s were generated if shapes didn't match for stable decoding. (default: True)

SACConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.normalization_mapping Dict
                        Mapping of feature types to normalization modes (default: {'VISUAL': <NormalizationMode.MEAN_STD: 'MEAN_STD'>, 'STATE': <NormalizationMode.MIN_MAX: 'MIN_MAX'>, 'ENV': <NormalizationMode.MIN_MAX: 'MIN_MAX'>, 'ACTION':
                        <NormalizationMode.MIN_MAX: 'MIN_MAX'>})
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device str   Architecture specifics Device to run the model on (e.g., "cuda", "cpu") (default: cpu)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed Precision (AMP) for training and evaluation. With AMP, automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub. (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.dataset_stats [Dict]
                        Statistics for normalizing different types of inputs (default: {'observation.image': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, 'observation.state': {'min': [0.0, 0.0], 'max': [1.0, 1.0]}, 'action': {'min': [0.0,
                        0.0, 0.0], 'max': [1.0, 1.0, 1.0]}})
  --policy.storage_device str
                        Device to store the model on (default: cpu)
  --policy.vision_encoder_name [str]
                        Name of the vision encoder model (Set to "helper2424/resnet10" for hil serl resnet10) (default: None)
  --policy.freeze_vision_encoder bool
                        Whether to freeze the vision encoder during training (default: True)
  --policy.image_encoder_hidden_dim int
                        Hidden dimension size for the image encoder (default: 32)
  --policy.shared_encoder bool
                        Whether to use a shared encoder for actor and critic (default: True)
  --policy.num_discrete_actions [int]
                        Number of discrete actions, eg for gripper actions (default: None)
  --policy.image_embedding_pooling_dim int
                        Dimension of the image embedding pooling (default: 8)
  --policy.online_steps int
                        Training parameter Number of steps for online training (default: 1000000)
  --policy.online_env_seed int
                        Seed for the online environment (default: 10000)
  --policy.online_buffer_capacity int
                        Capacity of the online replay buffer (default: 100000)
  --policy.offline_buffer_capacity int
                        Capacity of the offline replay buffer (default: 100000)
  --policy.async_prefetch bool
                        Whether to use asynchronous prefetching for the buffers (default: False)
  --policy.online_step_before_learning int
                        Number of steps before learning starts (default: 100)
  --policy.policy_update_freq int
                        Frequency of policy updates (default: 1)
  --policy.discount float
                        SAC algorithm parameters Discount factor for the SAC algorithm (default: 0.99)
  --policy.temperature_init float
                        Initial temperature value (default: 1.0)
  --policy.num_critics int
                        Number of critics in the ensemble (default: 2)
  --policy.num_subsample_critics [int]
                        Number of subsampled critics for training (default: None)
  --policy.critic_lr float
                        Learning rate for the critic network (default: 0.0003)
  --policy.actor_lr float
                        Learning rate for the actor network (default: 0.0003)
  --policy.temperature_lr float
                        Learning rate for the temperature parameter (default: 0.0003)
  --policy.critic_target_update_weight float
                        Weight for the critic target update (default: 0.005)
  --policy.utd_ratio int
                        Update-to-data ratio for the UTD algorithm (If you want enable utd_ratio, you need to set it to >1) (default: 1)
  --policy.state_encoder_hidden_dim int
                        Hidden dimension size for the state encoder (default: 256)
  --policy.latent_dim int
                        Dimension of the latent space (default: 256)
  --policy.target_entropy [float]
                        Target entropy for the SAC algorithm (default: None)
  --policy.use_backup_entropy bool
                        Whether to use backup entropy for the SAC algorithm (default: True)
  --policy.grad_clip_norm float
                        Gradient clipping norm for the SAC algorithm (default: 40.0)
  --policy.use_torch_compile bool
                        Optimizations (default: True)

CriticNetworkConfig ['policy.critic_network_kwargs']:
  Network configuration
  Configuration for the critic network architecture

  --policy.critic_network_kwargs.hidden_dims List
  --policy.critic_network_kwargs.activate_final bool
  --policy.critic_network_kwargs.final_activation [str]

ActorNetworkConfig ['policy.actor_network_kwargs']:
  Configuration for the actor network architecture

  --policy.actor_network_kwargs.hidden_dims List
  --policy.actor_network_kwargs.activate_final bool

PolicyConfig ['policy.policy_kwargs']:
  Configuration for the policy parameters

  --policy.policy_kwargs.use_tanh_squash bool
  --policy.policy_kwargs.std_min float
  --policy.policy_kwargs.std_max float
  --policy.policy_kwargs.init_final float

CriticNetworkConfig ['policy.discrete_critic_network_kwargs']:
  Configuration for the discrete critic network

  --policy.discrete_critic_network_kwargs.hidden_dims List
  --policy.discrete_critic_network_kwargs.activate_final bool
  --policy.discrete_critic_network_kwargs.final_activation [str]

ActorLearnerConfig ['policy.actor_learner_config']:
  Configuration for actor-learner architecture

  --policy.actor_learner_config.learner_host str
  --policy.actor_learner_config.learner_port int
  --policy.actor_learner_config.policy_parameters_push_frequency int
  --policy.actor_learner_config.queue_get_timeout float

ConcurrencyConfig ['policy.concurrency']:
  Configuration for concurrency settings (you can use threads or processes for the actor and learner)

  --policy.concurrency.actor str
  --policy.concurrency.learner str

RewardClassifierConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device str   
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed Precision (AMP) for training and evaluation. With AMP, automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub. (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.name str     
  --policy.num_classes int
  --policy.hidden_dim int
  --policy.latent_dim int
  --policy.image_embedding_pooling_dim int
  --policy.dropout_rate float
  --policy.model_name str
  --policy.model_type str
                        "transformer" or "cnn" (default: cnn)
  --policy.num_cameras int
  --policy.learning_rate float
  --policy.weight_decay float
  --policy.grad_clip_norm float

usage: record.py [-h] [--config_path str] [--robot str] [--robot.type {hope_jr_hand,hope_jr_arm,koch_follower,so100_follower,so100_follower_end_effector,so101_follower}] [--robot.side str] [--robot.urdf_path str] [--robot.target_frame_name str]
                 [--robot.end_effector_bounds str] [--robot.max_gripper_pos str] [--robot.end_effector_step_sizes str] [--robot.id str] [--robot.calibration_dir str] [--robot.port str] [--robot.disable_torque_on_disconnect str]
                 [--robot.max_relative_target str] [--robot.cameras str] [--robot.use_degrees str] [--dataset str] [--dataset.repo_id str] [--dataset.single_task str] [--dataset.root str] [--dataset.fps str] [--dataset.episode_time_s str]
                 [--dataset.reset_time_s str] [--dataset.num_episodes str] [--dataset.video str] [--dataset.push_to_hub str] [--dataset.private str] [--dataset.tags str] [--dataset.num_image_writer_processes str]
                 [--dataset.num_image_writer_threads_per_camera str] [--teleop str] [--teleop.type {homunculus_glove,homunculus_arm,koch_leader,so100_leader,so101_leader,keyboard,keyboard_ee}] [--teleop.side str] [--teleop.baud_rate str]
                 [--teleop.gripper_open_pos str] [--teleop.port str] [--teleop.use_degrees str] [--teleop.id str] [--teleop.calibration_dir str] [--teleop.mock str] [--teleop.use_gripper str] [--policy str]
                 [--policy.type {act,diffusion,pi0,smolvla,tdmpc,vqbet,pi0fast,sac,reward_classifier}] [--policy.replace_final_stride_with_dilation str] [--policy.pre_norm str] [--policy.dim_model str] [--policy.n_heads str] [--policy.dim_feedforward str]
                 [--policy.feedforward_activation str] [--policy.n_encoder_layers str] [--policy.n_decoder_layers str] [--policy.use_vae str] [--policy.n_vae_encoder_layers str] [--policy.temporal_ensemble_coeff str] [--policy.kl_weight str]
                 [--policy.optimizer_lr_backbone str] [--policy.drop_n_last_frames str] [--policy.use_separate_rgb_encoder_per_camera str] [--policy.down_dims str] [--policy.kernel_size str] [--policy.n_groups str] [--policy.diffusion_step_embed_dim str]
                 [--policy.use_film_scale_modulation str] [--policy.noise_scheduler_type str] [--policy.num_train_timesteps str] [--policy.beta_schedule str] [--policy.beta_start str] [--policy.beta_end str] [--policy.prediction_type str]
                 [--policy.clip_sample str] [--policy.clip_sample_range str] [--policy.num_inference_steps str] [--policy.do_mask_loss_for_padding str] [--policy.scheduler_name str] [--policy.attention_implementation str] [--policy.num_steps str]
                 [--policy.train_expert_only str] [--policy.train_state_proj str] [--policy.optimizer_grad_clip_norm str] [--policy.vlm_model_name str] [--policy.load_vlm_weights str] [--policy.add_image_special_tokens str] [--policy.attention_mode str]
                 [--policy.prefix_length str] [--policy.pad_language_to str] [--policy.num_expert_layers str] [--policy.num_vlm_layers str] [--policy.self_attn_every_n_layers str] [--policy.expert_width_multiplier str] [--policy.min_period str]
                 [--policy.max_period str] [--policy.n_action_repeats str] [--policy.horizon str] [--policy.q_ensemble_size str] [--policy.mlp_dim str] [--policy.use_mpc str] [--policy.cem_iterations str] [--policy.max_std str] [--policy.min_std str]
                 [--policy.n_gaussian_samples str] [--policy.n_pi_samples str] [--policy.uncertainty_regularizer_coeff str] [--policy.n_elites str] [--policy.elite_weighting_temperature str] [--policy.gaussian_mean_momentum str]
                 [--policy.max_random_shift_ratio str] [--policy.reward_coeff str] [--policy.expectile_weight str] [--policy.value_coeff str] [--policy.consistency_coeff str] [--policy.advantage_scaling str] [--policy.pi_coeff str]
                 [--policy.temporal_decay_coeff str] [--policy.target_model_momentum str] [--policy.n_action_pred_token str] [--policy.action_chunk_size str] [--policy.vision_backbone str] [--policy.crop_shape str] [--policy.crop_is_random str]
                 [--policy.pretrained_backbone_weights str] [--policy.use_group_norm str] [--policy.spatial_softmax_num_keypoints str] [--policy.n_vqvae_training_steps str] [--policy.vqvae_n_embed str] [--policy.vqvae_embedding_dim str]
                 [--policy.vqvae_enc_hidden_dim str] [--policy.gpt_block_size str] [--policy.gpt_input_dim str] [--policy.gpt_output_dim str] [--policy.gpt_n_layer str] [--policy.gpt_n_head str] [--policy.gpt_hidden_dim str] [--policy.dropout str]
                 [--policy.mlp_hidden_dim str] [--policy.offset_loss_weight str] [--policy.primary_code_loss_weight str] [--policy.secondary_code_loss_weight str] [--policy.bet_softmax_temperature str] [--policy.sequentially_select str]
                 [--policy.optimizer_vqvae_lr str] [--policy.optimizer_vqvae_weight_decay str] [--policy.chunk_size str] [--policy.n_action_steps str] [--policy.max_state_dim str] [--policy.max_action_dim str] [--policy.resize_imgs_with_padding str]
                 [--policy.interpolate_like_pi str] [--policy.empty_cameras str] [--policy.adapt_to_pi_aloha str] [--policy.use_delta_joint_actions_aloha str] [--policy.tokenizer_max_length str] [--policy.proj_width str] [--policy.max_decoding_steps str]
                 [--policy.fast_skip_tokens str] [--policy.max_input_seq_len str] [--policy.use_cache str] [--policy.freeze_lm_head str] [--policy.optimizer_lr str] [--policy.optimizer_betas str] [--policy.optimizer_eps str]
                 [--policy.optimizer_weight_decay str] [--policy.scheduler_warmup_steps str] [--policy.scheduler_decay_steps str] [--policy.scheduler_decay_lr str] [--policy.checkpoint_path str] [--policy.padding_side str] [--policy.precision str]
                 [--policy.relaxed_action_decoding str] [--policy.dataset_stats str] [--policy.storage_device str] [--policy.vision_encoder_name str] [--policy.freeze_vision_encoder str] [--policy.image_encoder_hidden_dim str] [--policy.shared_encoder str]
                 [--policy.num_discrete_actions str] [--policy.online_steps str] [--policy.online_env_seed str] [--policy.online_buffer_capacity str] [--policy.offline_buffer_capacity str] [--policy.async_prefetch str]
                 [--policy.online_step_before_learning str] [--policy.policy_update_freq str] [--policy.discount str] [--policy.temperature_init str] [--policy.num_critics str] [--policy.num_subsample_critics str] [--policy.critic_lr str]
                 [--policy.actor_lr str] [--policy.temperature_lr str] [--policy.critic_target_update_weight str] [--policy.utd_ratio str] [--policy.state_encoder_hidden_dim str] [--policy.target_entropy str] [--policy.use_backup_entropy str]
                 [--critic_network_kwargs str] [--policy.critic_network_kwargs.hidden_dims str] [--policy.critic_network_kwargs.activate_final str] [--policy.critic_network_kwargs.final_activation str] [--actor_network_kwargs str]
                 [--policy.actor_network_kwargs.hidden_dims str] [--policy.actor_network_kwargs.activate_final str] [--policy_kwargs str] [--policy.policy_kwargs.use_tanh_squash str] [--policy.policy_kwargs.std_min str] [--policy.policy_kwargs.std_max str]
                 [--policy.policy_kwargs.init_final str] [--discrete_critic_network_kwargs str] [--policy.discrete_critic_network_kwargs.hidden_dims str] [--policy.discrete_critic_network_kwargs.activate_final str]
                 [--policy.discrete_critic_network_kwargs.final_activation str] [--actor_learner_config str] [--policy.actor_learner_config.learner_host str] [--policy.actor_learner_config.learner_port str]
                 [--policy.actor_learner_config.policy_parameters_push_frequency str] [--policy.actor_learner_config.queue_get_timeout str] [--concurrency str] [--policy.concurrency.actor str] [--policy.concurrency.learner str]
                 [--policy.use_torch_compile str] [--policy.n_obs_steps str] [--policy.normalization_mapping str] [--policy.input_features str] [--policy.output_features str] [--policy.device str] [--policy.use_amp str] [--policy.push_to_hub str]
                 [--policy.repo_id str] [--policy.private str] [--policy.tags str] [--policy.license str] [--policy.name str] [--policy.num_classes str] [--policy.hidden_dim str] [--policy.latent_dim str] [--policy.image_embedding_pooling_dim str]
                 [--policy.dropout_rate str] [--policy.model_name str] [--policy.model_type str] [--policy.num_cameras str] [--policy.learning_rate str] [--policy.weight_decay str] [--policy.grad_clip_norm str] [--display_data str] [--play_sounds str]
                 [--resume str]